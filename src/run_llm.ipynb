{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import torch\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt_template = \"Identify keywords from the summary and description of the bug report that can be used to detect duplicates.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "project = 'spark'\n",
    "\n",
    "df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_content.csv')\n",
    "\n",
    "llama3_folder = '../data/keywords/{}/llama3/run_{}'\n",
    "\n",
    "for run in range(1, 6):\n",
    "    if not os.path.exists(llama3_folder.format(project, run)):\n",
    "        os.makedirs(llama3_folder.format(project, run))\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        bug_id = row['bug_id']\n",
    "        \n",
    "        if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(os.path.join(llama3_folder.format(project, run), f'{bug_id}.txt')):\n",
    "            continue\n",
    "                \n",
    "        messages = [\n",
    "            # {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt_template.format(row['short_desc'], row['description'])\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        prompt = pipeline.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        terminators = [\n",
    "            pipeline.tokenizer.eos_token_id,\n",
    "            pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        try:\n",
    "            outputs = pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=2048,\n",
    "                # max_length=2048,\n",
    "                eos_token_id=terminators,\n",
    "                do_sample=False,\n",
    "                top_p=1,\n",
    "            )\n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": prompt_template.format(row['short_desc'], row['description'][:2000])\n",
    "                    },\n",
    "                ]\n",
    "\n",
    "                prompt = pipeline.tokenizer.apply_chat_template(\n",
    "                        messages, \n",
    "                        tokenize=False, \n",
    "                        add_generation_prompt=True\n",
    "                )\n",
    "                \n",
    "                outputs = pipeline(\n",
    "                    prompt,\n",
    "                    max_new_tokens=2048,\n",
    "                    # max_length=2048,\n",
    "                    eos_token_id=terminators,\n",
    "                    do_sample=False,\n",
    "                    top_p=1,\n",
    "                )\n",
    "                \n",
    "        with open(os.path.join(llama3_folder.format(project, run), f'{bug_id}.txt'), 'w') as f:\n",
    "            f.write(prompt_template.format(row['short_desc'], row['description']))\n",
    "            f.write('\\n\\n>>>>>> Response:\\n\\n')\n",
    "            f.write(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3-mini-128k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import torch\n",
    "torch.random.manual_seed(42)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "prompt_template = \"Identify keywords from the summary and description of the bug report that can be used to detect duplicates.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "project = 'spark'\n",
    "\n",
    "df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_content.csv')\n",
    "\n",
    "phi_folder = '../data/keywords/{}/phi/run_{}'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "\n",
    "\n",
    "for run in range(1, 6):\n",
    "    if not os.path.exists(phi_folder.format(project, run)):\n",
    "        os.makedirs(phi_folder.format(project, run))\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        bug_id = row['bug_id']\n",
    "        \n",
    "        if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(os.path.join(phi_folder.format(project, run), f'{bug_id}.txt')):\n",
    "            continue\n",
    "                \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt_template.format(row['short_desc'], row['description'])\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "            )\n",
    "            \n",
    "            generation_args = {\n",
    "                \"max_new_tokens\": 2048,\n",
    "                \"return_full_text\": False,\n",
    "                \"temperature\": 0.0,\n",
    "                \"do_sample\": False,\n",
    "                \"top_p\": 1,\n",
    "            }\n",
    "\n",
    "            output = pipe(messages, **generation_args)\n",
    "            # print(output[0]['generated_text'])\n",
    "        \n",
    "        except Exception as e:\n",
    "                print(e)\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": prompt_template.format(row['short_desc'], row['description'][:2000])\n",
    "                    },\n",
    "                ]\n",
    "\n",
    "                pipe = pipeline(\n",
    "                    \"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                )\n",
    "                \n",
    "                generation_args = {\n",
    "                    \"max_new_tokens\": 2048,\n",
    "                    \"return_full_text\": False,\n",
    "                    \"temperature\": 0.0,\n",
    "                    \"do_sample\": False,\n",
    "                    \"top_p\": 1,\n",
    "                }\n",
    "\n",
    "                output = pipe(messages, **generation_args)\n",
    "                # print(output[0]['generated_text'])\n",
    "                \n",
    "        with open(os.path.join(phi_folder.format(project, run), f'{bug_id}.txt'), 'w') as f:\n",
    "            f.write(prompt_template.format(row['short_desc'], row['description']))\n",
    "            f.write('\\n\\n>>>>>> Response:\\n\\n')\n",
    "            f.write(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
