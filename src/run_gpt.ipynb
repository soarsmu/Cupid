{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 KB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/tingzhang/.local/lib/python3.10/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: sniffio in /home/tingzhang/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/tingzhang/.local/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/tingzhang/.local/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/tingzhang/.local/lib/python3.10/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/tingzhang/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/tingzhang/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/tingzhang/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/tingzhang/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/tingzhang/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting pydantic-core==2.18.2\n",
      "  Downloading pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: pydantic-core, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.6.0 openai-1.30.1 pydantic-2.7.1 pydantic-core-2.18.2\n",
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "%env OPENAI_API_KEY=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\n",
    "from openai import OpenAI\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'Provide a concise prompt or template that can be used to identify keywords from the summary and description of a bug report. These keywords will be used as input to detect duplicate bug reports. The output format will be:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords].')\n"
     ]
    }
   ],
   "source": [
    "gpt_assistant_prompt = \"\"\n",
    "gpt_user_prompt = \"Provide a concise prompt or template that can be used to identify keywords from the summary and description of a bug report. These keywords will be used as input to detect duplicate bug reports. The output format will be:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords].\"\n",
    "gpt_prompt = gpt_assistant_prompt, gpt_user_prompt\n",
    "print(gpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Identify keywords from the summary and description of the bug report that can be used to detect duplicates.\n",
      "\n",
      "Output format:\n",
      "Summary: [Selected Keywords]\n",
      "Description: [Selected Keywords]\n"
     ]
    }
   ],
   "source": [
    "message=[\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": gpt_user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages = message,\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    frequency_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concise prompt: \n",
      "Identify keywords from the bug report to detect duplicates.\n",
      "\n",
      "Verbose prompt:\n",
      "Review the summary and description of the bug report to identify specific keywords that can be used as criteria for detecting duplicate reports. Consider the language used, technical terms, and any unique identifiers mentioned in the report. \n",
      "\n",
      "Output format:\n",
      "Summary: [Selected Keywords]\n",
      "Description: [Selected Keywords]\n"
     ]
    }
   ],
   "source": [
    "alternative_prompt_template = \"Provide two alternative prompts: one is more concise, and the other is more verbose. The current prompt template is:\\n\\nIdentify keywords from the summary and description of the bug report that can be used to detect duplicates.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "\n",
    "client = OpenAI()\n",
    "message=[\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": alternative_prompt_template\n",
    "    }\n",
    "]\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages = message,\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    frequency_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [00:00<00:00, 3469.50it/s]\n",
      "100%|██████████| 2841/2841 [00:00<00:00, 3444.36it/s]\n",
      "100%|██████████| 2841/2841 [00:00<00:00, 3492.89it/s]\n",
      "100%|██████████| 2841/2841 [00:00<00:00, 3476.02it/s]\n",
      "100%|██████████| 2841/2841 [00:00<00:00, 3473.62it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"Identify keywords from the summary and description of the bug report that can be used to detect duplicates.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "\n",
    "for project in ['spark']:\n",
    "    df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "    flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_length_content.csv')\n",
    "\n",
    "    gpt_folder = '../data/keywords/{}/gpt/run_{}'\n",
    "    \n",
    "    for run in range(1, 6):\n",
    "        client = OpenAI()\n",
    "        \n",
    "        if not os.path.exists(gpt_folder.format(project, run)):\n",
    "            os.makedirs(gpt_folder.format(project, run))\n",
    "        \n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            bug_id = row['bug_id']\n",
    "            \n",
    "            if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            if os.path.exists(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt')):\n",
    "                continue\n",
    "            \n",
    "            message=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt_template.format(row['short_desc'], row['description'])\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages = message,\n",
    "                    temperature=0,\n",
    "                    top_p=1,\n",
    "                    max_tokens=2048,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0,\n",
    "                    seed=42\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                message = [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": prompt_template.format(row['short_desc'], row['description'][:2000])\n",
    "                    }\n",
    "                ]\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages = message,\n",
    "                    temperature=0,\n",
    "                    top_p=1,\n",
    "                    max_tokens=2048,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0,\n",
    "                    seed=42\n",
    "                )\n",
    "            \n",
    "            with open(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt'), 'w') as f:\n",
    "                f.write(prompt_template.format(row['short_desc'], row['description']))\n",
    "                f.write('\\n\\n>>>>>> Response:\\n\\n')\n",
    "                f.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [00:13<00:00, 203.06it/s]\n",
      "100%|██████████| 2841/2841 [00:09<00:00, 314.01it/s]\n",
      "100%|██████████| 2841/2841 [00:10<00:00, 263.45it/s]\n",
      "100%|██████████| 2841/2841 [00:11<00:00, 250.04it/s]\n",
      "100%|██████████| 2841/2841 [00:14<00:00, 194.68it/s]\n"
     ]
    }
   ],
   "source": [
    "concise_prompt = \"Identify keywords from the bug report to detect duplicates.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "\n",
    "project = 'spark'\n",
    "\n",
    "df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "gpt_folder = '../data/keywords/{}/gpt_concise/run_{}'\n",
    "flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_content.csv')\n",
    "\n",
    "for run in range(1, 6):\n",
    "    if not os.path.exists(gpt_folder.format(project, run)):\n",
    "        os.makedirs(gpt_folder.format(project, run))\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        bug_id = row['bug_id']\n",
    "        \n",
    "        if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "            if os.path.exists(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt')):\n",
    "                os.remove(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt'))\n",
    "            continue\n",
    "            \n",
    "        if os.path.exists(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt')):\n",
    "            continue\n",
    "        \n",
    "        message=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": concise_prompt.format(row['short_desc'], row['description'])\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages = message,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed=42\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            message = [\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": concise_prompt.format(row['short_desc'], row['description'][:2000])\n",
    "                }\n",
    "            ]\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages = message,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed=42\n",
    "            )\n",
    "        \n",
    "        with open(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt'), 'w') as f:\n",
    "            f.write(concise_prompt.format(row['short_desc'], row['description']))\n",
    "            f.write('\\n\\n>>>>>> Response:\\n\\n')\n",
    "            f.write(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 62/2841 [00:12<08:03,  5.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 48355 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1444/2841 [05:21<07:22,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16901 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [10:15<00:00,  4.62it/s]\n",
      "  2%|▏         | 62/2841 [00:17<15:25,  3.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 48355 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1440/2841 [04:59<04:09,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16901 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [10:25<00:00,  4.55it/s]\n",
      " 51%|█████     | 1444/2841 [05:07<05:06,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16901 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [10:19<00:00,  4.59it/s]\n",
      "  2%|▏         | 62/2841 [00:15<10:54,  4.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 48355 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1444/2841 [06:19<09:19,  2.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16901 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [10:49<00:00,  4.38it/s]\n",
      "  2%|▏         | 51/2841 [00:09<07:22,  6.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 48355 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1443/2841 [04:49<04:39,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16901 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2841/2841 [10:14<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "verbose_prompt = \"Review the summary and description of the bug report to identify specific keywords that can be used as criteria for detecting duplicate reports. Consider the language used, technical terms, and any unique identifiers mentioned in the report.\\n\\nOutput format:\\nSummary: [Selected Keywords]\\nDescription: [Selected Keywords]\\n\\nSummary: {}\\nDescription: {}\\n\\n\"\n",
    "\n",
    "project = 'spark'\n",
    "\n",
    "df = pd.read_csv('../data/raw/test_{}.csv'.format(project))\n",
    "gpt_folder = '../data/keywords/{}/gpt_verbose/run_{}'\n",
    "flag_content_df = pd.read_csv(f'../data/ablation/test_{project}_flag_content.csv')\n",
    "\n",
    "client = OpenAI()\n",
    "for run in range(1, 6):\n",
    "    if not os.path.exists(gpt_folder.format(project, run)):\n",
    "        os.makedirs(gpt_folder.format(project, run))\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        bug_id = row['bug_id']\n",
    "        if flag_content_df[flag_content_df['bug_id'] == bug_id]['run_flag'].values[0] == 0:\n",
    "                continue\n",
    "            \n",
    "        if os.path.exists(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt')):\n",
    "            continue\n",
    "        \n",
    "        message=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": verbose_prompt.format(row['short_desc'], row['description'])\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages = message,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed=42\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            message = [\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": verbose_prompt.format(row['short_desc'], row['description'][:2000])\n",
    "                }\n",
    "            ]\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages = message,\n",
    "                temperature=0,\n",
    "                top_p=1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                seed=42\n",
    "            )\n",
    "        \n",
    "        with open(os.path.join(gpt_folder.format(project, run), f'{bug_id}.txt'), 'w') as f:\n",
    "            f.write(verbose_prompt.format(row['short_desc'], row['description']))\n",
    "            f.write('\\n\\n>>>>>> Response:\\n\\n')\n",
    "            f.write(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17016/17016 [00:00<00:00, 135201.71it/s]\n"
     ]
    }
   ],
   "source": [
    "project = 'kibana'\n",
    "\n",
    "with open('../data/raw/test_{}.txt'.format(project), 'r') as f:\n",
    "        test_data = f.readlines()\n",
    "        \n",
    "test_bug_ids = set()\n",
    "for bug_id in test_data[1].split():\n",
    "    test_bug_ids.add(bug_id)\n",
    "print(len(test_bug_ids))\n",
    "\n",
    "original_content_json = '../data/raw/{}.json'.format(project)\n",
    "\n",
    "with open(original_content_json, 'r') as f:\n",
    "    original_content_lines = f.readlines()\n",
    "\n",
    "bug_ids, short_descs, descriptions = [], [], []\n",
    "for line in tqdm(original_content_lines):\n",
    "    bug = json.loads(line)        \n",
    "    if bug['bug_id'] in test_bug_ids:\n",
    "        bug_ids.append(bug['bug_id'])\n",
    "        short_descs.append(bug['short_desc'])\n",
    "        descriptions.append(bug['description'])\n",
    "        \n",
    "test_data = pd.DataFrame({'bug_id': bug_ids, 'short_desc': short_descs, 'description': descriptions})\n",
    "test_data.to_csv('../data/raw/test_{}.csv'.format(project), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
